{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soroushmirzaei/text-processing-projects/blob/main/divan-mulana-text-generation/divan-mulana-text-generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIYC55FdPdC2"
      },
      "outputs": [],
      "source": [
        "#import requirement libraries\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "#import dataset query libraries\n",
        "import csv\n",
        "import json\n",
        "\n",
        "#import mathematics statics libraries\n",
        "import random as rnd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#import machine learning deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SkBIh-h6ZzE"
      },
      "outputs": [],
      "source": [
        "#download filters-characters dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/english-language-filter-characters.txt\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-filter-characters.txt\n",
        "\n",
        "#download similar-characters dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-similar-characters.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cna6blsV2ZkZ"
      },
      "outputs": [],
      "source": [
        "#define filters-list function loader\n",
        "def filter_chars(file_path):\n",
        "    filter_chars = list()\n",
        "    with open(file_path, 'r') as filters_list_file:\n",
        "        for word in filters_list_file:\n",
        "            filter_chars.append(word.strip('\\n'))\n",
        "        filters_list_file.close()\n",
        "    return filter_chars\n",
        "\n",
        "#define similar-characters function loader\n",
        "def similar_chars(file_path):\n",
        "    with open(file_path, 'r') as similar_chars_file:\n",
        "        similar_chars = json.load(similar_chars_file)\n",
        "    return similar_chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLhC44LW6TAd"
      },
      "outputs": [],
      "source": [
        "#load filters-characters\n",
        "eng_filter_characters = filter_chars('english-language-filter-characters.txt')\n",
        "per_filter_characters = filter_chars('persian-language-filter-characters.txt')\n",
        "\n",
        "#load similar-characters\n",
        "per_similar_characters = similar_chars('persian-language-similar-characters.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39pSrmpdM5xg"
      },
      "outputs": [],
      "source": [
        "#define remove filters characters function\n",
        "def remove_filter(text, filters_list):\n",
        "    characters = list(text)\n",
        "    characters_without_filters = [character for character in characters if character not in filters_list]\n",
        "    text_without_filters = ''.join(characters_without_filters)\n",
        "    return text_without_filters\n",
        "\n",
        "#define similar characters modification function\n",
        "def similar_char(text, similar_chars_dict):\n",
        "    characters = list(text)\n",
        "    similar_characters_modified_list = [similar_chars_dict.get(character,character) for character in characters]\n",
        "    similar_characters_modified_text = ''.join(similar_characters_modified_list)\n",
        "    return similar_characters_modified_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs1sANpkK_xq"
      },
      "outputs": [],
      "source": [
        "#define texts and labels list loader for csv and json files\n",
        "def texts_loader(#define file path and type\n",
        "                 file_path, file_type,\n",
        "                 #define csv and txt files index for text and labels\n",
        "                 text_index = None, header_row = True, spliter_delimiter = None,\n",
        "                 #define json file keys for texts and labels\n",
        "                 text_key = None,\n",
        "                 #define preprocessing function for texts\n",
        "                 use_filter_remover = False, filters_list = None,\n",
        "                 use_similarchars_modifier = False, similarchars_dict = None\n",
        "                 ):\n",
        "    \n",
        "    #create empty texts labels list\n",
        "    texts_list = list()\n",
        "\n",
        "    #csv file loader\n",
        "    if file_type in ['csv']:\n",
        "        with open(file_path, 'r') as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter = spliter_delimiter)\n",
        "            if header_row:\n",
        "                next(csv_reader)\n",
        "            for row in csv_reader:\n",
        "                text = row[text_index]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                texts_list.append(text)\n",
        "        csv_file.close()\n",
        "\n",
        "    #txt file loader\n",
        "    if file_type in ['txt']:\n",
        "        with open(file_path, 'r') as txt_file:\n",
        "            for line in txt_file:\n",
        "                line = line.split(spliter_delimiter)\n",
        "                text = line[text_index]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                texts_list.append(text.strip('\\n'))\n",
        "        txt_file.close()\n",
        "    \n",
        "    #json file loader\n",
        "    if file_type in ['json']:\n",
        "        with open(file_path, 'r') as json_file:\n",
        "            json_reader = json.load(json_file)\n",
        "            for item in json_reader:\n",
        "                text = item[text_key]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                texts_list.append(text)\n",
        "        json_file.close()\n",
        "\n",
        "    return texts_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = texts_loader(#define file path and type\n",
        "                     file_path = '/content/divan-mulana-jalal-adin-rumi', file_type = 'txt',\n",
        "                     #define csv and txt files index for text and labels\n",
        "                     text_index = 0, spliter_delimiter = '\\n',\n",
        "                     use_filter_remover = True, filters_list = eng_filter_characters,\n",
        "                     )"
      ],
      "metadata": {
        "id": "5j1v9kzdisxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2SU7J1FjSKe",
        "outputId": "d9eb08b6-69ed-4030-adf0-e5435e283ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Light',\n",
              " '',\n",
              " 'I',\n",
              " '',\n",
              " '  Until the glorious Sun hath vanquished Night',\n",
              " '  The Birds of Day cower trembling with affright',\n",
              " '      But lo a bright glance bids the Tulip ope',\n",
              " '  O Heart awake thou too in Dutys might',\n",
              " '      The Suns Sword sheds in reddening flush of Dawn',\n",
              " '  The Blood of Night and puts the Foe to flight']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xws6QVTWPwbW"
      },
      "outputs": [],
      "source": [
        "#define tokenizer and sequences and padding sequences\n",
        "def texts_labels_generator(#define texts\n",
        "                           texts_list,\n",
        "                           #define filter characters list\n",
        "                           use_modified_filters = False, filters_list = None,\n",
        "                           #define json tokenizer\n",
        "                           save_tokenizer_json = False, tokenizer_filepath = None\n",
        "                           ):\n",
        "    \n",
        "    #define tokenizer filters and fit on texts\n",
        "    from keras.preprocessing.text import Tokenizer\n",
        "    if use_modified_filters:\n",
        "        filters = ''.join(filters_list)\n",
        "    else:\n",
        "        filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "    tokenizer = Tokenizer(filters = filters)\n",
        "    tokenizer.fit_on_texts(texts_list)\n",
        "\n",
        "    #define word_index\n",
        "    word_index = tokenizer.word_index\n",
        "    #for padding and counting out of vocab word\n",
        "    total_words = len(word_index) + 1\n",
        "\n",
        "    #save tokenizer json file\n",
        "    if save_tokenizer_json:\n",
        "        with open(tokenizer_filepath+'.json','w') as tokenizer_file:\n",
        "            json.dump(tokenizer.to_json(), tokenizer_file)\n",
        "\n",
        "    #define texts to sequences\n",
        "    texts_sequences = tokenizer.texts_to_sequences(texts_list)\n",
        "\n",
        "    #define phrase based sequences\n",
        "    phrases_sequences = list()\n",
        "\n",
        "    for text_sequence in texts_sequences:\n",
        "        for token_iter in range(1, len(text_sequence)):\n",
        "            phrase_sequence = text_sequence[:token_iter+1]\n",
        "            phrases_sequences.append(phrase_sequence)\n",
        "\n",
        "    #define maximum length of the sequences\n",
        "    maxlen = max([len(sequence) for sequence in phrases_sequences])\n",
        "\n",
        "    #define training validation pad sequences\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "    padded_sequences = pad_sequences(phrases_sequences, maxlen = maxlen, padding = 'pre')\n",
        "\n",
        "    #split texts and labels\n",
        "    texts = padded_sequences[:,:-1]\n",
        "    labels = padded_sequences[:,-1]\n",
        "\n",
        "    return texts, labels, maxlen, tokenizer, word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts, labels, maxlen, tokenizer, word_index = texts_labels_generator(#define texts\n",
        "                                                                      texts_list = texts,\n",
        "                                                                      #define filter characters list\n",
        "                                                                      use_modified_filters = False, filters_list = None,\n",
        "                                                                      #define json tokenizer\n",
        "                                                                      save_tokenizer_json = False, tokenizer_filepath = None)\n"
      ],
      "metadata": {
        "id": "wZEZqql-jcGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGSfyqvijnDh",
        "outputId": "ebf78fd5-62e4-42af-bda2-41a3ce80ed93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0, 209],\n",
              "       [  0,   0,   0, ...,   0, 209,   1],\n",
              "       [  0,   0,   0, ..., 209,   1, 750],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   0,   0,   7],\n",
              "       [  0,   0,   0, ...,   0,   7, 102],\n",
              "       [  0,   0,   0, ...,   7, 102,   3]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlUz5XBTkXzd",
        "outputId": "e04322c7-1578-44cc-c04b-39e6fc84323c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1, 750,  48, ..., 102,   3,  14], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ3p8PBDbFH2"
      },
      "outputs": [],
      "source": [
        "#define labels encoder\n",
        "def label_encoder(#define labels list and method\n",
        "                  labels_list,\n",
        "                  #define method binary, ordinal or onehot\n",
        "                  method, return_categories = True\n",
        "                  ):\n",
        "    \n",
        "    #ordinal and binary encoder method\n",
        "    if method in ['binary','ordinal']:\n",
        "        unique_labels = sorted(list(set(labels_list)))\n",
        "        labels_dict = {\n",
        "            label : int(unique_labels.index(label)) for label in unique_labels\n",
        "        }\n",
        "        labels = list(map(lambda label : labels_dict[label], labels_list))\n",
        "    \n",
        "    #one-hot encoder method\n",
        "    elif method in ['onehot']:\n",
        "        unique_labels = sorted(list(set(labels_list)))\n",
        "        labels_dict = {\n",
        "            label : int(unique_labels.index(label)) for label in unique_labels\n",
        "        }\n",
        "        labels_encoded = list()\n",
        "        for label in labels_list:\n",
        "            label_encoded = len(unique_labels)*[0]\n",
        "            label_number = labels_dict[label]\n",
        "            label_encoded[label_number] = 1\n",
        "            labels_encoded.append(label_encoded)\n",
        "        labels = labels_encoded\n",
        "\n",
        "    #convert list type to array\n",
        "    labels_encoded = np.array(labels)\n",
        "    \n",
        "    if return_categories:\n",
        "        return labels_encoded, labels_dict\n",
        "    else:\n",
        "        return labels_encoded\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_encoded, labels_dict = label_encoder(#define labels list and method\n",
        "                                            labels_list = labels,\n",
        "                                            #define method binary, ordinal or onehot\n",
        "                                            method = 'onehot', return_categories = True\n",
        "                                            )\n"
      ],
      "metadata": {
        "id": "VYsK1ZRQkk-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxWfT2f9k3Yl",
        "outputId": "e5ee2363-544c-4e7b-fbc2-3df09b2024ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_encoded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRHnC1EAk5ke",
        "outputId": "b1dfd9bd-ef22-4957-ab3e-1ee2f20cb657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6631, 1983)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define pre-trained words dictionary loader\n",
        "def word_dict_loader(#define file path and file type\n",
        "                     file_path, file_type,\n",
        "                     #define txt and csv file type args\n",
        "                     word_index = None, vector_index = None, header = True, spliter_delimiter = None,\n",
        "                     use_word_spliter = False, word_spliter = None, word_split_index = None,\n",
        "                     #define json file type args\n",
        "                     word_key = None, vector_key = None,\n",
        "                     ):\n",
        "    \n",
        "    word_dict = dict()\n",
        "\n",
        "    #define txt vec loader\n",
        "    if file_type in ['txt', 'vec']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            if header:\n",
        "                next(word_dict_file)\n",
        "            for row in word_dict_file:\n",
        "                row = row.split(spliter_delimiter)\n",
        "                if use_word_spliter:\n",
        "                    word = row[word_index].split(word_spliter)[word_split_index]\n",
        "                else:\n",
        "                    word = row[word_index]\n",
        "                vectors = np.array(row[vector_index:], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "\n",
        "    #define csv loader\n",
        "    elif file_type in ['csv']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            word_dict_file = csv.reader(word_dict_file, delimiter = spliter_delimiter)\n",
        "            if header:\n",
        "                next(word_dict_file)\n",
        "            for row in word_dict_file:\n",
        "                if use_word_spliter:\n",
        "                    word = row[word_index].split(word_spliter)[word_split_index]\n",
        "                else:\n",
        "                    word = row[word_index]\n",
        "                vectors = np.array(row[vector_index:], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "                \n",
        "    #define json loader\n",
        "    elif file_type in ['json']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            word_dict_file = json.load(word_dict_file)\n",
        "            for item in word_dict_file:\n",
        "                word = item[word_key]\n",
        "                vectors = np.array(item[vector_key], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "\n",
        "    #word dict params\n",
        "    word_dict_size = len(word_dict)\n",
        "    word_dict_dim = list(word_dict.values())[0].shape[0]\n",
        "\n",
        "    return word_dict, word_dict_size, word_dict_dim\n"
      ],
      "metadata": {
        "id": "brRWvKPSp4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define pre-trained embedding word vectors\n",
        "def embd_weights_loader(#define word dictionary and word index\n",
        "                        word_dict, word_index, dimension\n",
        "                        ):\n",
        "    \n",
        "    #create embedding weights\n",
        "    embed_weights = np.zeros([len(word_index)+1, dimension])\n",
        "\n",
        "    for word, index in word_index.items():\n",
        "        if word in word_dict:\n",
        "            embed_weights[index] = word_dict[word]\n",
        "\n",
        "    #embedding layer params\n",
        "    vocab_size = embed_weights.shape[0]\n",
        "    embed_dim = embed_weights.shape[1]\n",
        "\n",
        "    return embed_weights, vocab_size, embed_dim\n"
      ],
      "metadata": {
        "id": "fFr4DSbYpYjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlRqac9W2DWL"
      },
      "outputs": [],
      "source": [
        "#define model\n",
        "def create_model(#define input shape\n",
        "                 input_shape = None,\n",
        "                 #define embedding layer parameters\n",
        "                 use_pretraind_embd = False, vocab_size = None, embd_dim = None,\n",
        "                 sequence_len = None, embed_weights = None,\n",
        "                 #define type of layer and parameters\n",
        "                 use_lstm = False, use_gru = False, use_conv = False,\n",
        "                 #define lstm layers parameters\n",
        "                 lstm_layers_num = None, lstm_layers_units = None,\n",
        "                 #define gru layers parameters\n",
        "                 gru_layers_num = None, gru_layers_units = None,\n",
        "                 #define convolution layers parameters\n",
        "                 conv_layers_num = None, conv_layers_filters = None, conv_layers_kernel = None,\n",
        "                 #define convolution layers sub layers\n",
        "                 use_max_pool = False, max_pool_size = None,\n",
        "                 #define dense layer feeder\n",
        "                 use_global_max_pool = False, use_global_avg_pool = False, use_flatten = False,\n",
        "                 use_feeder_dropout = False, feeder_dropout_ratio = None,\n",
        "                 #define dense head layers\n",
        "                 use_dense_layer = False, dense_layers_num = None, dense_layers_units = None,\n",
        "                 #define dense layers dropout parameters\n",
        "                 use_dense_dropout = False, dense_dropout_ratio = None,\n",
        "                 #define output layer parameters\n",
        "                 output_layer_unit = None, output_layer_activation = None,\n",
        "                 #define model compiler parameters\n",
        "                 optimizer = None, loss = None, metrics = None\n",
        "                 ):\n",
        "    \n",
        "    #define input layer\n",
        "    input = keras.Input(shape = input_shape)\n",
        "\n",
        "    #define embedding layer and parameters\n",
        "    if use_pretraind_embd:\n",
        "        out = keras.layers.Embedding(input_dim = vocab_size, output_dim = embd_dim, input_length = sequence_len,\n",
        "                                     weights = [embed_weights], trainable = False)(input)\n",
        "    else:\n",
        "        out = keras.layers.Embedding(input_dim = vocab_size, output_dim = embd_dim, input_length = sequence_len)(input)\n",
        "\n",
        "    #define type of layer and parameters\n",
        "    #lstm type layers\n",
        "    if use_lstm:\n",
        "        sequence_return = (lstm_layers_num - 1)*[True]\n",
        "        sequence_return.append(False)\n",
        "        for layer_num in range(lstm_layers_num):\n",
        "            out = keras.layers.Bidirectional(keras.layers.LSTM(lstm_layers_units[layer_num],\n",
        "                                                               return_sequences = sequence_return[layer_num]))(out)\n",
        "\n",
        "    #gru type layers\n",
        "    elif use_gru:\n",
        "        sequence_return = (gru_layers_num - 1)*[True]\n",
        "        sequence_return.append(False)\n",
        "        for layer_num in range(gru_layers_num):\n",
        "            out = keras.layers.Bidirectional(keras.layers.GRU(gru_layers_units[layer_num],\n",
        "                                                              return_sequences = sequence_return[layer_num]))(out)\n",
        "    \n",
        "    #convolution type layer\n",
        "    elif use_conv:\n",
        "        for layer_num in range(conv_layers_num):\n",
        "            out = keras.layers.Conv1D(filters = conv_layers_filters[layer_num], kernel_size = conv_layers_kernel[layer_num],\n",
        "                                      activation = 'relu')(out)\n",
        "            if use_max_pool[layer_num]:\n",
        "                out = keras.layers.MaxPool1D(max_pool_size[layer_num])(out)\n",
        "\n",
        "\n",
        "    #dense layers feeder layer\n",
        "    #global max pool type layer\n",
        "    if use_global_max_pool:\n",
        "        out = keras.layers.GlobalMaxPooling1D()(out)\n",
        "        \n",
        "    #global average pool type layer\n",
        "    elif use_global_avg_pool:\n",
        "        out = keras.layers.GlobalAveragePooling1D()(out)\n",
        "\n",
        "    #flatten type layer\n",
        "    elif use_flatten:\n",
        "        out = keras.layers.Flatten()(out)\n",
        "\n",
        "    #define feeder dropout layer\n",
        "    if use_feeder_dropout:\n",
        "        out = keras.layers.Dropout(feeder_dropout_ratio)(out)\n",
        "\n",
        "\n",
        "    #define dense head layers\n",
        "    if use_dense_layer:\n",
        "        for layer_num in range(dense_layers_num):\n",
        "            out = keras.layers.Dense(dense_layers_units[layer_num], activation = 'relu')(out)\n",
        "            if use_dense_dropout[layer_num]:\n",
        "                out = keras.layers.Dropout(dense_dropout_ratio[layer_num])(out)\n",
        "    \n",
        "    #define output layer\n",
        "    output = keras.layers.Dense(output_layer_unit, activation = output_layer_activation)(out)\n",
        "\n",
        "    #define model\n",
        "    model = keras.models.Model(inputs = input, outputs = output)\n",
        "\n",
        "\n",
        "    #compile model\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss = loss,\n",
        "                  metrics = metrics)\n",
        "    \n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(#define input shape\n",
        "                     input_shape = maxlen - 1,\n",
        "                     #define embedding layer parameters\n",
        "                     use_pretraind_embd = False, vocab_size = len(word_index) + 1 , embd_dim = 128,\n",
        "                     sequence_len = maxlen - 1, embed_weights = None,\n",
        "                     #define type of layer and parameters\n",
        "                     use_lstm = True, use_gru = False, use_conv = False,\n",
        "                     #define lstm layers parameters\n",
        "                     lstm_layers_num = 2, lstm_layers_units = [64, 128],\n",
        "                     #define gru layers parameters\n",
        "                     gru_layers_num = None, gru_layers_units = None,\n",
        "                     #define convolution layers parameters\n",
        "                     conv_layers_num = None, conv_layers_filters = None, conv_layers_kernel = None,\n",
        "                     #define convolution layers sub layers\n",
        "                     use_max_pool = False, max_pool_size = None,\n",
        "                     #define dense layer feeder\n",
        "                     use_global_max_pool = False, use_global_avg_pool = False, use_flatten = False,\n",
        "                     use_feeder_dropout = True, feeder_dropout_ratio = 0.2,\n",
        "                     #define dense head layers\n",
        "                     dense_layers_num = None, dense_layers_units = None,\n",
        "                     #define dense layers dropout parameters\n",
        "                     use_dense_dropout = False, dense_dropout_ratio = None,\n",
        "                     #define output layer parameters\n",
        "                     output_layer_unit = len(labels_dict), output_layer_activation = 'softmax',\n",
        "                     #define model compiler parameters\n",
        "                     optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'accuracy'\n",
        "                     )"
      ],
      "metadata": {
        "id": "7cApqHwGmYYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model summary\n",
        "model.summary(120)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoM66pE6oxL0",
        "outputId": "3d57b84d-edb5-4827-ea3e-425ab7eacec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "________________________________________________________________________________________________________________________\n",
            " Layer (type)                                         Output Shape                                    Param #           \n",
            "========================================================================================================================\n",
            " input_1 (InputLayer)                                 [(None, 14)]                                    0                 \n",
            "                                                                                                                        \n",
            " embedding (Embedding)                                (None, 14, 128)                                 270592            \n",
            "                                                                                                                        \n",
            " bidirectional (Bidirectional)                        (None, 14, 128)                                 98816             \n",
            "                                                                                                                        \n",
            " bidirectional_1 (Bidirectional)                      (None, 256)                                     263168            \n",
            "                                                                                                                        \n",
            " dropout (Dropout)                                    (None, 256)                                     0                 \n",
            "                                                                                                                        \n",
            " dense (Dense)                                        (None, 1983)                                    509631            \n",
            "                                                                                                                        \n",
            "========================================================================================================================\n",
            "Total params: 1,142,207\n",
            "Trainable params: 1,142,207\n",
            "Non-trainable params: 0\n",
            "________________________________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model\n",
        "model.fit(texts, labels_encoded, epochs = 500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQcBis-no21v",
        "outputId": "eafb73cb-c6d4-429a-966f-ead2abd2d8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "208/208 [==============================] - 14s 11ms/step - loss: 6.8365 - accuracy: 0.0511\n",
            "Epoch 2/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 6.3973 - accuracy: 0.0543\n",
            "Epoch 3/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 6.2050 - accuracy: 0.0588\n",
            "Epoch 4/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 6.0146 - accuracy: 0.0674\n",
            "Epoch 5/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 5.8344 - accuracy: 0.0772\n",
            "Epoch 6/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 5.6451 - accuracy: 0.0864\n",
            "Epoch 7/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 5.4609 - accuracy: 0.0992\n",
            "Epoch 8/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 5.2722 - accuracy: 0.1096\n",
            "Epoch 9/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 5.0763 - accuracy: 0.1178\n",
            "Epoch 10/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 4.8769 - accuracy: 0.1267\n",
            "Epoch 11/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 4.6875 - accuracy: 0.1407\n",
            "Epoch 12/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 4.5122 - accuracy: 0.1523\n",
            "Epoch 13/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 4.3346 - accuracy: 0.1654\n",
            "Epoch 14/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 4.1743 - accuracy: 0.1767\n",
            "Epoch 15/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 4.0290 - accuracy: 0.1887\n",
            "Epoch 16/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 3.8693 - accuracy: 0.2155\n",
            "Epoch 17/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 3.7245 - accuracy: 0.2331\n",
            "Epoch 18/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 3.5801 - accuracy: 0.2538\n",
            "Epoch 19/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 3.4556 - accuracy: 0.2761\n",
            "Epoch 20/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 3.3207 - accuracy: 0.3009\n",
            "Epoch 21/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 3.1912 - accuracy: 0.3205\n",
            "Epoch 22/500\n",
            "208/208 [==============================] - 3s 15ms/step - loss: 3.0718 - accuracy: 0.3440\n",
            "Epoch 23/500\n",
            "208/208 [==============================] - 3s 16ms/step - loss: 2.9563 - accuracy: 0.3672\n",
            "Epoch 24/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.8436 - accuracy: 0.3850\n",
            "Epoch 25/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.7212 - accuracy: 0.4076\n",
            "Epoch 26/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.6314 - accuracy: 0.4325\n",
            "Epoch 27/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.5121 - accuracy: 0.4471\n",
            "Epoch 28/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.4193 - accuracy: 0.4711\n",
            "Epoch 29/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.3423 - accuracy: 0.4805\n",
            "Epoch 30/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.2284 - accuracy: 0.5087\n",
            "Epoch 31/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.1406 - accuracy: 0.5266\n",
            "Epoch 32/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 2.0546 - accuracy: 0.5435\n",
            "Epoch 33/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.9828 - accuracy: 0.5602\n",
            "Epoch 34/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.9024 - accuracy: 0.5809\n",
            "Epoch 35/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.8307 - accuracy: 0.5987\n",
            "Epoch 36/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.7606 - accuracy: 0.6108\n",
            "Epoch 37/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.7066 - accuracy: 0.6206\n",
            "Epoch 38/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.6350 - accuracy: 0.6382\n",
            "Epoch 39/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.5829 - accuracy: 0.6450\n",
            "Epoch 40/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.5194 - accuracy: 0.6578\n",
            "Epoch 41/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.4725 - accuracy: 0.6768\n",
            "Epoch 42/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.4098 - accuracy: 0.6857\n",
            "Epoch 43/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.3769 - accuracy: 0.6957\n",
            "Epoch 44/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.3250 - accuracy: 0.7056\n",
            "Epoch 45/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.2690 - accuracy: 0.7098\n",
            "Epoch 46/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.2245 - accuracy: 0.7267\n",
            "Epoch 47/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.1848 - accuracy: 0.7359\n",
            "Epoch 48/500\n",
            "208/208 [==============================] - 3s 13ms/step - loss: 1.1473 - accuracy: 0.7408\n",
            "Epoch 49/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.1169 - accuracy: 0.7460\n",
            "Epoch 50/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.0926 - accuracy: 0.7581\n",
            "Epoch 51/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.0487 - accuracy: 0.7629\n",
            "Epoch 52/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 1.0391 - accuracy: 0.7619\n",
            "Epoch 53/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.9977 - accuracy: 0.7718\n",
            "Epoch 54/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.9712 - accuracy: 0.7798\n",
            "Epoch 55/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.9266 - accuracy: 0.7880\n",
            "Epoch 56/500\n",
            "208/208 [==============================] - 3s 15ms/step - loss: 0.9093 - accuracy: 0.7899\n",
            "Epoch 57/500\n",
            "208/208 [==============================] - 4s 18ms/step - loss: 0.8929 - accuracy: 0.7928\n",
            "Epoch 58/500\n",
            "208/208 [==============================] - 4s 20ms/step - loss: 0.8734 - accuracy: 0.7996\n",
            "Epoch 59/500\n",
            "208/208 [==============================] - 3s 15ms/step - loss: 0.8345 - accuracy: 0.8097\n",
            "Epoch 60/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.8293 - accuracy: 0.8115\n",
            "Epoch 61/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.8038 - accuracy: 0.8130\n",
            "Epoch 62/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7900 - accuracy: 0.8153\n",
            "Epoch 63/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7648 - accuracy: 0.8227\n",
            "Epoch 64/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7553 - accuracy: 0.8183\n",
            "Epoch 65/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7347 - accuracy: 0.8257\n",
            "Epoch 66/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7255 - accuracy: 0.8255\n",
            "Epoch 67/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7086 - accuracy: 0.8322\n",
            "Epoch 68/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.7027 - accuracy: 0.8331\n",
            "Epoch 69/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6911 - accuracy: 0.8334\n",
            "Epoch 70/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6969 - accuracy: 0.8319\n",
            "Epoch 71/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6831 - accuracy: 0.8361\n",
            "Epoch 72/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6699 - accuracy: 0.8376\n",
            "Epoch 73/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6577 - accuracy: 0.8401\n",
            "Epoch 74/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6407 - accuracy: 0.8427\n",
            "Epoch 75/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6324 - accuracy: 0.8454\n",
            "Epoch 76/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6196 - accuracy: 0.8468\n",
            "Epoch 77/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6051 - accuracy: 0.8506\n",
            "Epoch 78/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6042 - accuracy: 0.8475\n",
            "Epoch 79/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.6032 - accuracy: 0.8506\n",
            "Epoch 80/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5920 - accuracy: 0.8524\n",
            "Epoch 81/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5818 - accuracy: 0.8533\n",
            "Epoch 82/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5960 - accuracy: 0.8515\n",
            "Epoch 83/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5683 - accuracy: 0.8563\n",
            "Epoch 84/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5700 - accuracy: 0.8522\n",
            "Epoch 85/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5682 - accuracy: 0.8554\n",
            "Epoch 86/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5625 - accuracy: 0.8584\n",
            "Epoch 87/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5708 - accuracy: 0.8522\n",
            "Epoch 88/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5469 - accuracy: 0.8594\n",
            "Epoch 89/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5359 - accuracy: 0.8602\n",
            "Epoch 90/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5373 - accuracy: 0.8584\n",
            "Epoch 91/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5364 - accuracy: 0.8575\n",
            "Epoch 92/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5577 - accuracy: 0.8564\n",
            "Epoch 93/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5420 - accuracy: 0.8617\n",
            "Epoch 94/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5285 - accuracy: 0.8594\n",
            "Epoch 95/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5301 - accuracy: 0.8608\n",
            "Epoch 96/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5212 - accuracy: 0.8643\n",
            "Epoch 97/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5081 - accuracy: 0.8658\n",
            "Epoch 98/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4989 - accuracy: 0.8671\n",
            "Epoch 99/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4926 - accuracy: 0.8673\n",
            "Epoch 100/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5025 - accuracy: 0.8661\n",
            "Epoch 101/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4931 - accuracy: 0.8696\n",
            "Epoch 102/500\n",
            "208/208 [==============================] - 3s 15ms/step - loss: 0.5027 - accuracy: 0.8643\n",
            "Epoch 103/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5133 - accuracy: 0.8625\n",
            "Epoch 104/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5080 - accuracy: 0.8653\n",
            "Epoch 105/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5048 - accuracy: 0.8643\n",
            "Epoch 106/500\n",
            "208/208 [==============================] - 3s 13ms/step - loss: 0.5004 - accuracy: 0.8665\n",
            "Epoch 107/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5124 - accuracy: 0.8625\n",
            "Epoch 108/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4969 - accuracy: 0.8646\n",
            "Epoch 109/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4996 - accuracy: 0.8617\n",
            "Epoch 110/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.5028 - accuracy: 0.8617\n",
            "Epoch 111/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4776 - accuracy: 0.8705\n",
            "Epoch 112/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4742 - accuracy: 0.8709\n",
            "Epoch 113/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4916 - accuracy: 0.8649\n",
            "Epoch 114/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4914 - accuracy: 0.8649\n",
            "Epoch 115/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4836 - accuracy: 0.8673\n",
            "Epoch 116/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4741 - accuracy: 0.8700\n",
            "Epoch 117/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4733 - accuracy: 0.8693\n",
            "Epoch 118/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4734 - accuracy: 0.8685\n",
            "Epoch 119/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4735 - accuracy: 0.8689\n",
            "Epoch 120/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4714 - accuracy: 0.8686\n",
            "Epoch 121/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4599 - accuracy: 0.8726\n",
            "Epoch 122/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4760 - accuracy: 0.8686\n",
            "Epoch 123/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4631 - accuracy: 0.8708\n",
            "Epoch 124/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4838 - accuracy: 0.8656\n",
            "Epoch 125/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4789 - accuracy: 0.8662\n",
            "Epoch 126/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4734 - accuracy: 0.8676\n",
            "Epoch 127/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4847 - accuracy: 0.8655\n",
            "Epoch 128/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4773 - accuracy: 0.8667\n",
            "Epoch 129/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4808 - accuracy: 0.8661\n",
            "Epoch 130/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4706 - accuracy: 0.8668\n",
            "Epoch 131/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4647 - accuracy: 0.8670\n",
            "Epoch 132/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4570 - accuracy: 0.8711\n",
            "Epoch 133/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4477 - accuracy: 0.8724\n",
            "Epoch 134/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4564 - accuracy: 0.8696\n",
            "Epoch 135/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4474 - accuracy: 0.8757\n",
            "Epoch 136/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4505 - accuracy: 0.8711\n",
            "Epoch 137/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4523 - accuracy: 0.8691\n",
            "Epoch 138/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4549 - accuracy: 0.8682\n",
            "Epoch 139/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4576 - accuracy: 0.8709\n",
            "Epoch 140/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4725 - accuracy: 0.8671\n",
            "Epoch 141/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4659 - accuracy: 0.8676\n",
            "Epoch 142/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4563 - accuracy: 0.8708\n",
            "Epoch 143/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4696 - accuracy: 0.8694\n",
            "Epoch 144/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4584 - accuracy: 0.8683\n",
            "Epoch 145/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4485 - accuracy: 0.8702\n",
            "Epoch 146/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4495 - accuracy: 0.8724\n",
            "Epoch 147/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4492 - accuracy: 0.8705\n",
            "Epoch 148/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4459 - accuracy: 0.8699\n",
            "Epoch 149/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4414 - accuracy: 0.8727\n",
            "Epoch 150/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4530 - accuracy: 0.8699\n",
            "Epoch 151/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4635 - accuracy: 0.8682\n",
            "Epoch 152/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4584 - accuracy: 0.8679\n",
            "Epoch 153/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4694 - accuracy: 0.8679\n",
            "Epoch 154/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4626 - accuracy: 0.8668\n",
            "Epoch 155/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4591 - accuracy: 0.8694\n",
            "Epoch 156/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4412 - accuracy: 0.8714\n",
            "Epoch 157/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4379 - accuracy: 0.8727\n",
            "Epoch 158/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4348 - accuracy: 0.8757\n",
            "Epoch 159/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4369 - accuracy: 0.8718\n",
            "Epoch 160/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4316 - accuracy: 0.8732\n",
            "Epoch 161/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4374 - accuracy: 0.8739\n",
            "Epoch 162/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4398 - accuracy: 0.8733\n",
            "Epoch 163/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4351 - accuracy: 0.8747\n",
            "Epoch 164/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4637 - accuracy: 0.8662\n",
            "Epoch 165/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4581 - accuracy: 0.8711\n",
            "Epoch 166/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4598 - accuracy: 0.8674\n",
            "Epoch 167/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4389 - accuracy: 0.8723\n",
            "Epoch 168/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4569 - accuracy: 0.8665\n",
            "Epoch 169/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4459 - accuracy: 0.8696\n",
            "Epoch 170/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4291 - accuracy: 0.8733\n",
            "Epoch 171/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4293 - accuracy: 0.8726\n",
            "Epoch 172/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4345 - accuracy: 0.8718\n",
            "Epoch 173/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4311 - accuracy: 0.8708\n",
            "Epoch 174/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4311 - accuracy: 0.8741\n",
            "Epoch 175/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4391 - accuracy: 0.8718\n",
            "Epoch 176/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4342 - accuracy: 0.8708\n",
            "Epoch 177/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4306 - accuracy: 0.8729\n",
            "Epoch 178/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4436 - accuracy: 0.8711\n",
            "Epoch 179/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4566 - accuracy: 0.8677\n",
            "Epoch 180/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4513 - accuracy: 0.8711\n",
            "Epoch 181/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4423 - accuracy: 0.8715\n",
            "Epoch 182/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4352 - accuracy: 0.8735\n",
            "Epoch 183/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4384 - accuracy: 0.8676\n",
            "Epoch 184/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4303 - accuracy: 0.8738\n",
            "Epoch 185/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4308 - accuracy: 0.8721\n",
            "Epoch 186/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4280 - accuracy: 0.8753\n",
            "Epoch 187/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4266 - accuracy: 0.8736\n",
            "Epoch 188/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4387 - accuracy: 0.8738\n",
            "Epoch 189/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4297 - accuracy: 0.8735\n",
            "Epoch 190/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4242 - accuracy: 0.8721\n",
            "Epoch 191/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4420 - accuracy: 0.8714\n",
            "Epoch 192/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4532 - accuracy: 0.8662\n",
            "Epoch 193/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4454 - accuracy: 0.8658\n",
            "Epoch 194/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4433 - accuracy: 0.8700\n",
            "Epoch 195/500\n",
            "208/208 [==============================] - 3s 14ms/step - loss: 0.4368 - accuracy: 0.8730\n",
            "Epoch 196/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4260 - accuracy: 0.8744\n",
            "Epoch 197/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4261 - accuracy: 0.8733\n",
            "Epoch 198/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4165 - accuracy: 0.8756\n",
            "Epoch 199/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4253 - accuracy: 0.8727\n",
            "Epoch 200/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4192 - accuracy: 0.8744\n",
            "Epoch 201/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4169 - accuracy: 0.8748\n",
            "Epoch 202/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4218 - accuracy: 0.8732\n",
            "Epoch 203/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4240 - accuracy: 0.8714\n",
            "Epoch 204/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4233 - accuracy: 0.8738\n",
            "Epoch 205/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4219 - accuracy: 0.8729\n",
            "Epoch 206/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4330 - accuracy: 0.8708\n",
            "Epoch 207/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4462 - accuracy: 0.8680\n",
            "Epoch 208/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4440 - accuracy: 0.8685\n",
            "Epoch 209/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4461 - accuracy: 0.8671\n",
            "Epoch 210/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4292 - accuracy: 0.8729\n",
            "Epoch 211/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4274 - accuracy: 0.8711\n",
            "Epoch 212/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4212 - accuracy: 0.8727\n",
            "Epoch 213/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4230 - accuracy: 0.8747\n",
            "Epoch 214/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4320 - accuracy: 0.8732\n",
            "Epoch 215/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4131 - accuracy: 0.8747\n",
            "Epoch 216/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4231 - accuracy: 0.8726\n",
            "Epoch 217/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4193 - accuracy: 0.8729\n",
            "Epoch 218/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4214 - accuracy: 0.8733\n",
            "Epoch 219/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4255 - accuracy: 0.8714\n",
            "Epoch 220/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4237 - accuracy: 0.8726\n",
            "Epoch 221/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4232 - accuracy: 0.8721\n",
            "Epoch 222/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4309 - accuracy: 0.8739\n",
            "Epoch 223/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4315 - accuracy: 0.8726\n",
            "Epoch 224/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.4237 - accuracy: 0.8732\n",
            "Epoch 225/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4187 - accuracy: 0.8735\n",
            "Epoch 226/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4154 - accuracy: 0.8750\n",
            "Epoch 227/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4172 - accuracy: 0.8732\n",
            "Epoch 228/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4127 - accuracy: 0.8741\n",
            "Epoch 229/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4284 - accuracy: 0.8709\n",
            "Epoch 230/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4312 - accuracy: 0.8703\n",
            "Epoch 231/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4335 - accuracy: 0.8694\n",
            "Epoch 232/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4276 - accuracy: 0.8717\n",
            "Epoch 233/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4159 - accuracy: 0.8753\n",
            "Epoch 234/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4194 - accuracy: 0.8730\n",
            "Epoch 235/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4187 - accuracy: 0.8751\n",
            "Epoch 236/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4153 - accuracy: 0.8753\n",
            "Epoch 237/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4162 - accuracy: 0.8723\n",
            "Epoch 238/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4375 - accuracy: 0.8688\n",
            "Epoch 239/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4236 - accuracy: 0.8733\n",
            "Epoch 240/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4168 - accuracy: 0.8735\n",
            "Epoch 241/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4161 - accuracy: 0.8739\n",
            "Epoch 242/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4103 - accuracy: 0.8757\n",
            "Epoch 243/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4131 - accuracy: 0.8741\n",
            "Epoch 244/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4074 - accuracy: 0.8774\n",
            "Epoch 245/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4130 - accuracy: 0.8726\n",
            "Epoch 246/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4102 - accuracy: 0.8724\n",
            "Epoch 247/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4071 - accuracy: 0.8759\n",
            "Epoch 248/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4199 - accuracy: 0.8738\n",
            "Epoch 249/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4341 - accuracy: 0.8689\n",
            "Epoch 250/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4329 - accuracy: 0.8706\n",
            "Epoch 251/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4201 - accuracy: 0.8714\n",
            "Epoch 252/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4209 - accuracy: 0.8720\n",
            "Epoch 253/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4189 - accuracy: 0.8757\n",
            "Epoch 254/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4259 - accuracy: 0.8683\n",
            "Epoch 255/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4189 - accuracy: 0.8724\n",
            "Epoch 256/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4190 - accuracy: 0.8729\n",
            "Epoch 257/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4103 - accuracy: 0.8762\n",
            "Epoch 258/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4083 - accuracy: 0.8745\n",
            "Epoch 259/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4095 - accuracy: 0.8745\n",
            "Epoch 260/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4102 - accuracy: 0.8750\n",
            "Epoch 261/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4094 - accuracy: 0.8744\n",
            "Epoch 262/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4122 - accuracy: 0.8733\n",
            "Epoch 263/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4209 - accuracy: 0.8729\n",
            "Epoch 264/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4224 - accuracy: 0.8732\n",
            "Epoch 265/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4138 - accuracy: 0.8739\n",
            "Epoch 266/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4153 - accuracy: 0.8736\n",
            "Epoch 267/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4085 - accuracy: 0.8751\n",
            "Epoch 268/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4072 - accuracy: 0.8766\n",
            "Epoch 269/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4122 - accuracy: 0.8741\n",
            "Epoch 270/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4070 - accuracy: 0.8732\n",
            "Epoch 271/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4089 - accuracy: 0.8760\n",
            "Epoch 272/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4013 - accuracy: 0.8742\n",
            "Epoch 273/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4153 - accuracy: 0.8739\n",
            "Epoch 274/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4068 - accuracy: 0.8763\n",
            "Epoch 275/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4128 - accuracy: 0.8739\n",
            "Epoch 276/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4104 - accuracy: 0.8750\n",
            "Epoch 277/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4198 - accuracy: 0.8723\n",
            "Epoch 278/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4330 - accuracy: 0.8674\n",
            "Epoch 279/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4255 - accuracy: 0.8696\n",
            "Epoch 280/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4144 - accuracy: 0.8729\n",
            "Epoch 281/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4139 - accuracy: 0.8726\n",
            "Epoch 282/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4081 - accuracy: 0.8735\n",
            "Epoch 283/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4036 - accuracy: 0.8751\n",
            "Epoch 284/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4068 - accuracy: 0.8733\n",
            "Epoch 285/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4092 - accuracy: 0.8741\n",
            "Epoch 286/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4118 - accuracy: 0.8727\n",
            "Epoch 287/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4018 - accuracy: 0.8771\n",
            "Epoch 288/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4033 - accuracy: 0.8754\n",
            "Epoch 289/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4063 - accuracy: 0.8739\n",
            "Epoch 290/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4098 - accuracy: 0.8739\n",
            "Epoch 291/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4090 - accuracy: 0.8735\n",
            "Epoch 292/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4104 - accuracy: 0.8730\n",
            "Epoch 293/500\n",
            "208/208 [==============================] - 3s 14ms/step - loss: 0.4096 - accuracy: 0.8730\n",
            "Epoch 294/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4009 - accuracy: 0.8777\n",
            "Epoch 295/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4075 - accuracy: 0.8744\n",
            "Epoch 296/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4136 - accuracy: 0.8714\n",
            "Epoch 297/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4065 - accuracy: 0.8741\n",
            "Epoch 298/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4063 - accuracy: 0.8735\n",
            "Epoch 299/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4051 - accuracy: 0.8754\n",
            "Epoch 300/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4000 - accuracy: 0.8760\n",
            "Epoch 301/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4039 - accuracy: 0.8733\n",
            "Epoch 302/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4110 - accuracy: 0.8732\n",
            "Epoch 303/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4172 - accuracy: 0.8717\n",
            "Epoch 304/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4173 - accuracy: 0.8717\n",
            "Epoch 305/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4190 - accuracy: 0.8732\n",
            "Epoch 306/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4104 - accuracy: 0.8735\n",
            "Epoch 307/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4094 - accuracy: 0.8736\n",
            "Epoch 308/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4046 - accuracy: 0.8756\n",
            "Epoch 309/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4024 - accuracy: 0.8744\n",
            "Epoch 310/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4073 - accuracy: 0.8739\n",
            "Epoch 311/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4071 - accuracy: 0.8736\n",
            "Epoch 312/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4064 - accuracy: 0.8745\n",
            "Epoch 313/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4021 - accuracy: 0.8748\n",
            "Epoch 314/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4086 - accuracy: 0.8733\n",
            "Epoch 315/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4090 - accuracy: 0.8727\n",
            "Epoch 316/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4124 - accuracy: 0.8726\n",
            "Epoch 317/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4037 - accuracy: 0.8738\n",
            "Epoch 318/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4008 - accuracy: 0.8751\n",
            "Epoch 319/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4013 - accuracy: 0.8757\n",
            "Epoch 320/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4004 - accuracy: 0.8745\n",
            "Epoch 321/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4014 - accuracy: 0.8760\n",
            "Epoch 322/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4119 - accuracy: 0.8760\n",
            "Epoch 323/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4223 - accuracy: 0.8711\n",
            "Epoch 324/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4196 - accuracy: 0.8700\n",
            "Epoch 325/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4113 - accuracy: 0.8709\n",
            "Epoch 326/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4096 - accuracy: 0.8706\n",
            "Epoch 327/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4097 - accuracy: 0.8730\n",
            "Epoch 328/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4053 - accuracy: 0.8735\n",
            "Epoch 329/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3986 - accuracy: 0.8733\n",
            "Epoch 330/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3952 - accuracy: 0.8783\n",
            "Epoch 331/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3988 - accuracy: 0.8759\n",
            "Epoch 332/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3957 - accuracy: 0.8747\n",
            "Epoch 333/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3995 - accuracy: 0.8757\n",
            "Epoch 334/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3973 - accuracy: 0.8748\n",
            "Epoch 335/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3933 - accuracy: 0.8784\n",
            "Epoch 336/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4018 - accuracy: 0.8768\n",
            "Epoch 337/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3973 - accuracy: 0.8765\n",
            "Epoch 338/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3984 - accuracy: 0.8772\n",
            "Epoch 339/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4049 - accuracy: 0.8741\n",
            "Epoch 340/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4082 - accuracy: 0.8712\n",
            "Epoch 341/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4162 - accuracy: 0.8721\n",
            "Epoch 342/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4066 - accuracy: 0.8735\n",
            "Epoch 343/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4160 - accuracy: 0.8694\n",
            "Epoch 344/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4165 - accuracy: 0.8705\n",
            "Epoch 345/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4045 - accuracy: 0.8768\n",
            "Epoch 346/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4012 - accuracy: 0.8760\n",
            "Epoch 347/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4002 - accuracy: 0.8739\n",
            "Epoch 348/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3960 - accuracy: 0.8754\n",
            "Epoch 349/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3951 - accuracy: 0.8756\n",
            "Epoch 350/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3941 - accuracy: 0.8741\n",
            "Epoch 351/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4024 - accuracy: 0.8742\n",
            "Epoch 352/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4013 - accuracy: 0.8745\n",
            "Epoch 353/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4005 - accuracy: 0.8754\n",
            "Epoch 354/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3979 - accuracy: 0.8750\n",
            "Epoch 355/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4092 - accuracy: 0.8733\n",
            "Epoch 356/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.4163 - accuracy: 0.8715\n",
            "Epoch 357/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4026 - accuracy: 0.8759\n",
            "Epoch 358/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4007 - accuracy: 0.8766\n",
            "Epoch 359/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3939 - accuracy: 0.8768\n",
            "Epoch 360/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3953 - accuracy: 0.8756\n",
            "Epoch 361/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3993 - accuracy: 0.8747\n",
            "Epoch 362/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3966 - accuracy: 0.8769\n",
            "Epoch 363/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3968 - accuracy: 0.8733\n",
            "Epoch 364/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3984 - accuracy: 0.8753\n",
            "Epoch 365/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3975 - accuracy: 0.8727\n",
            "Epoch 366/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3979 - accuracy: 0.8757\n",
            "Epoch 367/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4055 - accuracy: 0.8723\n",
            "Epoch 368/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4027 - accuracy: 0.8735\n",
            "Epoch 369/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4072 - accuracy: 0.8748\n",
            "Epoch 370/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4010 - accuracy: 0.8738\n",
            "Epoch 371/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3998 - accuracy: 0.8741\n",
            "Epoch 372/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3991 - accuracy: 0.8747\n",
            "Epoch 373/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3989 - accuracy: 0.8750\n",
            "Epoch 374/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3959 - accuracy: 0.8745\n",
            "Epoch 375/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4028 - accuracy: 0.8732\n",
            "Epoch 376/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3968 - accuracy: 0.8757\n",
            "Epoch 377/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4051 - accuracy: 0.8745\n",
            "Epoch 378/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.4052 - accuracy: 0.8721\n",
            "Epoch 379/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4091 - accuracy: 0.8712\n",
            "Epoch 380/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3963 - accuracy: 0.8771\n",
            "Epoch 381/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3999 - accuracy: 0.8733\n",
            "Epoch 382/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3930 - accuracy: 0.8774\n",
            "Epoch 383/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3960 - accuracy: 0.8747\n",
            "Epoch 384/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3905 - accuracy: 0.8762\n",
            "Epoch 385/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3931 - accuracy: 0.8732\n",
            "Epoch 386/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3943 - accuracy: 0.8753\n",
            "Epoch 387/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3951 - accuracy: 0.8763\n",
            "Epoch 388/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3922 - accuracy: 0.8744\n",
            "Epoch 389/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3998 - accuracy: 0.8751\n",
            "Epoch 390/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4059 - accuracy: 0.8757\n",
            "Epoch 391/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.4054 - accuracy: 0.8742\n",
            "Epoch 392/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4009 - accuracy: 0.8762\n",
            "Epoch 393/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3987 - accuracy: 0.8732\n",
            "Epoch 394/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3977 - accuracy: 0.8744\n",
            "Epoch 395/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3967 - accuracy: 0.8772\n",
            "Epoch 396/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3982 - accuracy: 0.8729\n",
            "Epoch 397/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4007 - accuracy: 0.8720\n",
            "Epoch 398/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3990 - accuracy: 0.8742\n",
            "Epoch 399/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3975 - accuracy: 0.8745\n",
            "Epoch 400/500\n",
            "208/208 [==============================] - 3s 13ms/step - loss: 0.3982 - accuracy: 0.8742\n",
            "Epoch 401/500\n",
            "208/208 [==============================] - 3s 13ms/step - loss: 0.3888 - accuracy: 0.8751\n",
            "Epoch 402/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3929 - accuracy: 0.8759\n",
            "Epoch 403/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3907 - accuracy: 0.8788\n",
            "Epoch 404/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3959 - accuracy: 0.8750\n",
            "Epoch 405/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3941 - accuracy: 0.8729\n",
            "Epoch 406/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3955 - accuracy: 0.8750\n",
            "Epoch 407/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.4023 - accuracy: 0.8732\n",
            "Epoch 408/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4193 - accuracy: 0.8676\n",
            "Epoch 409/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4027 - accuracy: 0.8751\n",
            "Epoch 410/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3987 - accuracy: 0.8756\n",
            "Epoch 411/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3924 - accuracy: 0.8768\n",
            "Epoch 412/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3944 - accuracy: 0.8772\n",
            "Epoch 413/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3973 - accuracy: 0.8741\n",
            "Epoch 414/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3883 - accuracy: 0.8762\n",
            "Epoch 415/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3946 - accuracy: 0.8757\n",
            "Epoch 416/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3925 - accuracy: 0.8765\n",
            "Epoch 417/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3928 - accuracy: 0.8759\n",
            "Epoch 418/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3981 - accuracy: 0.8744\n",
            "Epoch 419/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4061 - accuracy: 0.8732\n",
            "Epoch 420/500\n",
            "208/208 [==============================] - 3s 13ms/step - loss: 0.3979 - accuracy: 0.8730\n",
            "Epoch 421/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3876 - accuracy: 0.8754\n",
            "Epoch 422/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3882 - accuracy: 0.8763\n",
            "Epoch 423/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3930 - accuracy: 0.8751\n",
            "Epoch 424/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3880 - accuracy: 0.8772\n",
            "Epoch 425/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3915 - accuracy: 0.8748\n",
            "Epoch 426/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3903 - accuracy: 0.8765\n",
            "Epoch 427/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3881 - accuracy: 0.8769\n",
            "Epoch 428/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3889 - accuracy: 0.8753\n",
            "Epoch 429/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3908 - accuracy: 0.8766\n",
            "Epoch 430/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3939 - accuracy: 0.8741\n",
            "Epoch 431/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3935 - accuracy: 0.8751\n",
            "Epoch 432/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3896 - accuracy: 0.8775\n",
            "Epoch 433/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3950 - accuracy: 0.8738\n",
            "Epoch 434/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3922 - accuracy: 0.8759\n",
            "Epoch 435/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3928 - accuracy: 0.8762\n",
            "Epoch 436/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3954 - accuracy: 0.8742\n",
            "Epoch 437/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3995 - accuracy: 0.8723\n",
            "Epoch 438/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4295 - accuracy: 0.8670\n",
            "Epoch 439/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4096 - accuracy: 0.8726\n",
            "Epoch 440/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3975 - accuracy: 0.8726\n",
            "Epoch 441/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3915 - accuracy: 0.8750\n",
            "Epoch 442/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3915 - accuracy: 0.8760\n",
            "Epoch 443/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3923 - accuracy: 0.8765\n",
            "Epoch 444/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3874 - accuracy: 0.8760\n",
            "Epoch 445/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3839 - accuracy: 0.8756\n",
            "Epoch 446/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3852 - accuracy: 0.8757\n",
            "Epoch 447/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3886 - accuracy: 0.8750\n",
            "Epoch 448/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3837 - accuracy: 0.8774\n",
            "Epoch 449/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3919 - accuracy: 0.8738\n",
            "Epoch 450/500\n",
            "208/208 [==============================] - 2s 11ms/step - loss: 0.3900 - accuracy: 0.8751\n",
            "Epoch 451/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3871 - accuracy: 0.8780\n",
            "Epoch 452/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3918 - accuracy: 0.8760\n",
            "Epoch 453/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3971 - accuracy: 0.8738\n",
            "Epoch 454/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.4021 - accuracy: 0.8714\n",
            "Epoch 455/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3962 - accuracy: 0.8745\n",
            "Epoch 456/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3939 - accuracy: 0.8760\n",
            "Epoch 457/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3916 - accuracy: 0.8735\n",
            "Epoch 458/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3935 - accuracy: 0.8720\n",
            "Epoch 459/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3880 - accuracy: 0.8769\n",
            "Epoch 460/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3937 - accuracy: 0.8742\n",
            "Epoch 461/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3907 - accuracy: 0.8760\n",
            "Epoch 462/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3896 - accuracy: 0.8753\n",
            "Epoch 463/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3850 - accuracy: 0.8774\n",
            "Epoch 464/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3931 - accuracy: 0.8736\n",
            "Epoch 465/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3890 - accuracy: 0.8745\n",
            "Epoch 466/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3958 - accuracy: 0.8715\n",
            "Epoch 467/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3956 - accuracy: 0.8745\n",
            "Epoch 468/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.4025 - accuracy: 0.8723\n",
            "Epoch 469/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4037 - accuracy: 0.8733\n",
            "Epoch 470/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3933 - accuracy: 0.8742\n",
            "Epoch 471/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3940 - accuracy: 0.8726\n",
            "Epoch 472/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3854 - accuracy: 0.8788\n",
            "Epoch 473/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3905 - accuracy: 0.8748\n",
            "Epoch 474/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3856 - accuracy: 0.8753\n",
            "Epoch 475/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3889 - accuracy: 0.8747\n",
            "Epoch 476/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3877 - accuracy: 0.8750\n",
            "Epoch 477/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3880 - accuracy: 0.8753\n",
            "Epoch 478/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3874 - accuracy: 0.8781\n",
            "Epoch 479/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3861 - accuracy: 0.8775\n",
            "Epoch 480/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3817 - accuracy: 0.8774\n",
            "Epoch 481/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3835 - accuracy: 0.8756\n",
            "Epoch 482/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3882 - accuracy: 0.8757\n",
            "Epoch 483/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3922 - accuracy: 0.8732\n",
            "Epoch 484/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3878 - accuracy: 0.8763\n",
            "Epoch 485/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3881 - accuracy: 0.8742\n",
            "Epoch 486/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4000 - accuracy: 0.8730\n",
            "Epoch 487/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3958 - accuracy: 0.8738\n",
            "Epoch 488/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3892 - accuracy: 0.8739\n",
            "Epoch 489/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.4055 - accuracy: 0.8727\n",
            "Epoch 490/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3986 - accuracy: 0.8706\n",
            "Epoch 491/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3936 - accuracy: 0.8745\n",
            "Epoch 492/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3917 - accuracy: 0.8759\n",
            "Epoch 493/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3835 - accuracy: 0.8750\n",
            "Epoch 494/500\n",
            "208/208 [==============================] - 3s 14ms/step - loss: 0.3893 - accuracy: 0.8754\n",
            "Epoch 495/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3855 - accuracy: 0.8760\n",
            "Epoch 496/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3807 - accuracy: 0.8759\n",
            "Epoch 497/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3873 - accuracy: 0.8763\n",
            "Epoch 498/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3825 - accuracy: 0.8757\n",
            "Epoch 499/500\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.3859 - accuracy: 0.8759\n",
            "Epoch 500/500\n",
            "208/208 [==============================] - 3s 12ms/step - loss: 0.3859 - accuracy: 0.8754\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e51a78a50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot model training loss\n",
        "pd.DataFrame(model.history.history)[['loss']].plot(figsize = (9, 6), linewidth = 3)\n",
        "plt.grid(linestyle = '--', linewidth = 2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "r9WlvK5vzfSG",
        "outputId": "15ad5265-ab10-464e-c7f7-9a2f6d4e48db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot model training accuracy\n",
        "pd.DataFrame(model.history.history)[['accuracy']].plot(figsize = (9, 6), linewidth = 3)\n",
        "plt.grid(linestyle = '--', linewidth = 2)\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "v4gmV_f1z1nM",
        "outputId": "f6edbcd4-2a2e-44c5-8dfa-4434cd92cf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model evaluation\n",
        "train_set_eval = model.evaluate(texts, labels_encoded, verbose = 0)\n",
        "print(f'Training Set Evaluation:\\n\\tLoss: {round(train_set_eval[0],4)}\\tAccuracy: {100*round(train_set_eval[1],4)}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1fGgl5a0Ms8",
        "outputId": "c3e37f79-fe22-4db7-ed28-c7a5bb4c142e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Evaluation:\n",
            "\tLoss: 0.3418\tAccuracy: 88.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define text generator based on model\n",
        "def text_generator(#define text and numbert of iteration\n",
        "                   input_text, iteration,\n",
        "                   #define tokenizer and maxlen\n",
        "                   tokenizer, maxlen, word_index,\n",
        "                   #define labels categories and model\n",
        "                   labels_dict, model, \n",
        "                   ):\n",
        "    \n",
        "    input_text = str(input_text)\n",
        "    key_values_labels = dict([(key,value) for value, key in labels_dict.items()])\n",
        "    index_word = dict([(key, value) for value, key in word_index.items()])\n",
        "\n",
        "    for iter in range(iteration):\n",
        "        text = tokenizer.texts_to_sequences([input_text])\n",
        "        text = keras.preprocessing.sequence.pad_sequences(text, padding = 'pre', maxlen = maxlen - 1)\n",
        "        predict = model.predict(text)\n",
        "        predict = np.argmax(predict, axis = -1)[0]\n",
        "        predict = key_values_labels[predict]\n",
        "        predict = index_word[predict]\n",
        "        input_text = input_text + \" \" + predict\n",
        "    \n",
        "    return input_text.title()\n"
      ],
      "metadata": {
        "id": "4veh4dxp72m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate the text\n",
        "generated_text = text_generator(#define text and numbert of iteration\n",
        "                                input_text = 'We Never Die In Heart', iteration = 20,\n",
        "                                #define tokenizer and maxlen\n",
        "                                tokenizer = tokenizer, maxlen = maxlen, word_index = word_index,\n",
        "                                #define labels categories and model\n",
        "                                labels_dict = labels_dict, model = model,\n",
        "                                )\n",
        "#print out the generated text\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmfAFEdhLQKq",
        "outputId": "71ec84ab-98d6-462f-ee98-56cf95d11e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We Never Die In Heart All Dreaming And Thought You There My Flame Hallelu Parting Close Look And East One God And Up And East\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "model.save('/content/model.h5')\n"
      ],
      "metadata": {
        "id": "gikdU4aO0Lgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "divan-mulana-text-generation.ipynb",
      "provenance": [],
      "mount_file_id": "1YqkKxxHYU3J4ituglLL786xGYjnko5FS",
      "authorship_tag": "ABX9TyO2kSRKozbu1yiGRNk95dM5",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}